{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement A Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting Mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting Mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting Mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 1\tCost: 0.332\t\tAccuracy: 0.902\t\tTime: 8.574\t\n",
      "Epoch: 2\tCost: 0.077\t\tAccuracy: 0.953\t\tTime: 8.662\t\n",
      "Epoch: 3\tCost: 0.048\t\tAccuracy: 0.966\t\tTime: 9.565\t\n",
      "Epoch: 4\tCost: 0.041\t\tAccuracy: 0.973\t\tTime: 9.201\t\n",
      "Epoch: 5\tCost: 0.041\t\tAccuracy: 0.978\t\tTime: 9.072\t\n",
      "Epoch: 6\tCost: 0.042\t\tAccuracy: 0.981\t\tTime: 8.980\t\n",
      "Epoch: 7\tCost: 0.039\t\tAccuracy: 0.985\t\tTime: 9.289\t\n",
      "Epoch: 8\tCost: 0.031\t\tAccuracy: 0.987\t\tTime: 9.250\t\n",
      "Epoch: 9\tCost: 0.023\t\tAccuracy: 0.989\t\tTime: 9.354\t\n",
      "Epoch: 10\tCost: 0.016\t\tAccuracy: 0.990\t\tTime: 9.342\t\n",
      "Epoch: 11\tCost: 0.012\t\tAccuracy: 0.992\t\tTime: 9.443\t\n",
      "Epoch: 12\tCost: 0.007\t\tAccuracy: 0.993\t\tTime: 9.747\t\n",
      "Epoch: 13\tCost: 0.006\t\tAccuracy: 0.994\t\tTime: 9.799\t\n",
      "Epoch: 14\tCost: 0.005\t\tAccuracy: 0.996\t\tTime: 10.270\t\n",
      "Epoch: 15\tCost: 0.003\t\tAccuracy: 0.996\t\tTime: 10.055\t\n",
      "Epoch: 16\tCost: 0.002\t\tAccuracy: 0.997\t\tTime: 10.294\t\n",
      "Epoch: 17\tCost: 0.002\t\tAccuracy: 0.998\t\tTime: 9.624\t\n",
      "Epoch: 18\tCost: 0.002\t\tAccuracy: 0.998\t\tTime: 9.534\t\n",
      "Epoch: 19\tCost: 0.001\t\tAccuracy: 0.999\t\tTime: 9.595\t\n",
      "Epoch: 20\tCost: 0.001\t\tAccuracy: 0.999\t\tTime: 9.683\t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"Mnist_data/\", one_hot=True)\n",
    "IMGS = mnist.train.images\n",
    "LABS = mnist.train.labels\n",
    "IMGS_Test = mnist.test.images\n",
    "LABS_Test = mnist.test.labels\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "def softmax_prime(z):\n",
    "    return softmax(z) * (1 - softmax(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "# def relu_prime(z):\n",
    "#     result = []\n",
    "#     for t in z:\n",
    "#         result.append(float(t > 0))\n",
    "#     return np.array(result, ndmin=2).T\n",
    "\n",
    "# This one is a lot faster\n",
    "def relu_prime(z):\n",
    "    x = np.array(z)\n",
    "    return np.ceil((abs(x)+x)/2/100)\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.2\n",
    "batch_size = 64\n",
    "n = mnist.train.num_examples\n",
    "total_batch = int(n/batch_size)\n",
    "n_step = epochs * total_batch\n",
    "\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 0.00000001\n",
    "\n",
    "n_input = 784\n",
    "n_hidden = 100\n",
    "n_output = 10\n",
    "\n",
    "w1 = np.random.normal(0, 0.1 ,(n_hidden, n_input))\n",
    "b1 = np.random.normal(0, 0.1 ,(n_hidden, 1))\n",
    "w2 = np.random.normal(0, 0.1 ,(n_output, n_hidden))\n",
    "b2 = np.random.normal(0, 0.1 ,(n_output, 1))\n",
    "\n",
    "for j in range(epochs):\n",
    "    time_start = time.time()\n",
    "    error = 0\n",
    "    error_t = 0\n",
    "    cost = 0\n",
    "    for k in range(0,n,batch_size):\n",
    "        imgs = IMGS[k:k+batch_size]\n",
    "        labels = LABS[k:k+batch_size]\n",
    "        imgs_test = IMGS_Test\n",
    "        labels_test = LABS_Test\n",
    "        w1_gd = 0\n",
    "        w2_gd = 0\n",
    "        b1_gd = 0\n",
    "        b2_gd = 0\n",
    "        t = 0\n",
    "        a2_m = 0\n",
    "        a1_m = 0\n",
    "        a2_v = 0\n",
    "        a1_v = 0\n",
    "        a2_m_b = 0\n",
    "        a1_m_b = 0\n",
    "        a2_v_b = 0\n",
    "        a1_v_b = 0\n",
    "\n",
    "        for x, y in zip(imgs, labels):\n",
    "            x = np.array(x, ndmin=2).T\n",
    "            y = np.array(y, ndmin=2).T\n",
    "            z1 = np.dot(w1, x) + b1\n",
    "            a1 = relu(z1)\n",
    "            z2 = np.dot(w2, a1) + b2\n",
    "            a2 = softmax(z2)\n",
    "\n",
    "            d2 = a2 - y\n",
    "            d1 = relu_prime(z1)*np.dot(w2.T, d2)\n",
    "\n",
    "            w1_gd += np.dot(d1, x.T)\n",
    "            w2_gd += np.dot(d2, a1.T)\n",
    "            b1_gd += d1\n",
    "            b2_gd += d2\n",
    "            error += int(np.argmax(a2) != np.argmax(y))\n",
    "            cost = np.sum(y*np.log(a2) + (1-y)*np.log(1-a2))\n",
    "#             # Adam\n",
    "#             t += 1\n",
    "#             a2_m = a2_m * beta_1 + (1-beta_1)*w2_gd\n",
    "#             a1_m = a1_m * beta_1 + (1-beta_1)*w1_gd\n",
    "#             a2_m_b = a2_m_b * beta_1 + (1-beta_1)*b2_gd\n",
    "#             a1_m_b = a1_m_b * beta_1 + (1-beta_1)*b1_gd\n",
    "        \n",
    "#             a2_v = a2_v * beta_2 + (1-beta_2)* (w2_gd ** 2)\n",
    "#             a1_v = a1_v * beta_2 + (1-beta_2)* (w1_gd ** 2)\n",
    "#             a2_v_b = a2_v_b * beta_2 + (1-beta_2)* (b2_gd ** 2)\n",
    "#             a1_v_b = a1_v_b * beta_2 + (1-beta_2)* (b1_gd ** 2)\n",
    "        \n",
    "#             a2_m_nor = a2_m / (1 - (beta_1 ** t))\n",
    "#             a1_m_nor = a1_m / (1 - (beta_1 ** t))\n",
    "#             a2_v_nor = a2_v / (1 - (beta_2 ** t))\n",
    "#             a1_v_nor = a1_v / (1 - (beta_2 ** t))\n",
    "#             a2_m_b_nor = a2_m_b / (1 - (beta_1 ** t))\n",
    "#             a1_m_b_nor = a1_m_b / (1 - (beta_1 ** t))\n",
    "#             a2_v_b_nor = a2_v_b / (1 - (beta_2 ** t))\n",
    "#             a1_v_b_nor = a1_v_b / (1 - (beta_2 ** t))        \n",
    "#         w2 -= (a2_m_nor / (np.sqrt(a2_v_nor) + epsilon)) / batch_size\n",
    "#         w1 -= (a1_m_nor / (np.sqrt(a1_v_nor) + epsilon)) / batch_size\n",
    "#         b2 -= (a2_m_b_nor / (np.sqrt(a2_v_b_nor) + epsilon)) / batch_size\n",
    "#         b1 -= (a1_m_b_nor / (np.sqrt(a1_v_b_nor) + epsilon)) / batch_size\n",
    "        \n",
    "        # Gradient Descent\n",
    "        w1 -= lr * w1_gd / batch_size\n",
    "        w2 -= lr * w2_gd / batch_size\n",
    "        b1 -= lr * b1_gd / batch_size\n",
    "        b2 -= lr * b2_gd / batch_size\n",
    "    time_end = time.time()\n",
    "    print(\"Epoch: \"+str(j+1)+\"\\tCost: {:.3f}\\t\".format(-cost)+\"\\tAccuracy: {:.3f}\\t\".format(1 - error/n)+\n",
    "          \"\\tTime: {:.3f}\\t\".format(time_end-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.972\t\n"
     ]
    }
   ],
   "source": [
    "    n_test = mnist.test.num_examples\n",
    "    for x,y in zip(imgs_test, labels_test):\n",
    "        x = np.array(x, ndmin=2).T\n",
    "        y = np.array(y, ndmin=2).T\n",
    "        z1 = np.dot(w1, x) + b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(w2, a1) + b2\n",
    "        a2 = softmax(z2)\n",
    "        error_t += int(np.argmax(a2) != np.argmax(y))\n",
    "    print(\"Test Accuracy: {:.3f}\\t\".format(1 - error_t/n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e:  \n",
    "The cost and training time of each epoch is list in previous cells along with the test accuracy. To save space, only the result from batch size 64 is printed out. I summarized my result of different batch size in the following table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| batch size \t| cost of first epoch \t| average training time \t| test accuracy \t|\n",
    "|:----------:\t|:-------------------:\t|:---------------------:\t|:-------------:\t|\n",
    "|     16     \t|        0.001        \t|         10.022        \t|     0.977     \t|\n",
    "|     64     \t|        0.332        \t|         9.482         \t|     0.972     \t|\n",
    "|     256    \t|        0.860        \t|         9.064         \t|     0.971     \t|\n",
    "|    1024    \t|        0.943        \t|         8.626         \t|     0.951     \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table you can see that, as the batch size grows, the cost of first epoch is getting bigger. Besides, I found out cost almost converge to 0 after 20 epochs except batch size of 1024. When the batch size is 1024, even after 20 epochs of training, the cost is still 0.187. However, when it comes to training time, larger batch size has the advantage. I think this is because for smaller batch size, it will need iterate more times in one epoch, thus slowing down the speed. Since the accuracy is closely related to cost, it's obvious smaller batch size is better in test accuracy. Therefore, we need to choose a good batch size in training process to balance training speed and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part f:  \n",
    "Implement ADAM Optimizer and do the summary as part e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting Mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting Mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting Mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 1\tCost: 0.589\t\tAccuracy: 0.772\t\tTime: 33.853\t\n",
      "Epoch: 2\tCost: 0.384\t\tAccuracy: 0.904\t\tTime: 34.114\t\n",
      "Epoch: 3\tCost: 0.284\t\tAccuracy: 0.921\t\tTime: 35.328\t\n",
      "Epoch: 4\tCost: 0.240\t\tAccuracy: 0.932\t\tTime: 34.746\t\n",
      "Epoch: 5\tCost: 0.193\t\tAccuracy: 0.939\t\tTime: 35.626\t\n",
      "Epoch: 6\tCost: 0.159\t\tAccuracy: 0.945\t\tTime: 46.546\t\n",
      "Epoch: 7\tCost: 0.135\t\tAccuracy: 0.950\t\tTime: 38.990\t\n",
      "Epoch: 8\tCost: 0.120\t\tAccuracy: 0.954\t\tTime: 36.597\t\n",
      "Epoch: 9\tCost: 0.102\t\tAccuracy: 0.957\t\tTime: 36.108\t\n",
      "Epoch: 10\tCost: 0.088\t\tAccuracy: 0.960\t\tTime: 35.580\t\n",
      "Epoch: 11\tCost: 0.075\t\tAccuracy: 0.962\t\tTime: 35.199\t\n",
      "Epoch: 12\tCost: 0.061\t\tAccuracy: 0.964\t\tTime: 35.048\t\n",
      "Epoch: 13\tCost: 0.048\t\tAccuracy: 0.966\t\tTime: 35.089\t\n",
      "Epoch: 14\tCost: 0.038\t\tAccuracy: 0.967\t\tTime: 35.425\t\n",
      "Epoch: 15\tCost: 0.030\t\tAccuracy: 0.969\t\tTime: 38.514\t\n",
      "Epoch: 16\tCost: 0.023\t\tAccuracy: 0.971\t\tTime: 37.926\t\n",
      "Epoch: 17\tCost: 0.019\t\tAccuracy: 0.972\t\tTime: 35.642\t\n",
      "Epoch: 18\tCost: 0.015\t\tAccuracy: 0.973\t\tTime: 35.339\t\n",
      "Epoch: 19\tCost: 0.011\t\tAccuracy: 0.974\t\tTime: 36.055\t\n",
      "Epoch: 20\tCost: 0.008\t\tAccuracy: 0.976\t\tTime: 34.718\t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"Mnist_data/\", one_hot=True)\n",
    "IMGS = mnist.train.images\n",
    "LABS = mnist.train.labels\n",
    "IMGS_Test = mnist.test.images\n",
    "LABS_Test = mnist.test.labels\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "\n",
    "def softmax_prime(z):\n",
    "    return softmax(z) * (1 - softmax(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "# def relu_prime(z):\n",
    "#     result = []\n",
    "#     for t in z:\n",
    "#         result.append(float(t > 0))\n",
    "#     return np.array(result, ndmin=2).T\n",
    "\n",
    "# This one is a lot faster\n",
    "def relu_prime(z):\n",
    "    x = np.array(z)\n",
    "    return np.ceil((abs(x)+x)/2/100)\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.2\n",
    "batch_size = 1024\n",
    "n = mnist.train.num_examples\n",
    "total_batch = int(n/batch_size)\n",
    "n_step = epochs * total_batch\n",
    "\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 0.00000001\n",
    "\n",
    "n_input = 784\n",
    "n_hidden = 100\n",
    "n_output = 10\n",
    "\n",
    "w1 = np.random.normal(0, 0.1 ,(n_hidden, n_input))\n",
    "b1 = np.random.normal(0, 0.1 ,(n_hidden, 1))\n",
    "w2 = np.random.normal(0, 0.1 ,(n_output, n_hidden))\n",
    "b2 = np.random.normal(0, 0.1 ,(n_output, 1))\n",
    "\n",
    "for j in range(epochs):\n",
    "    time_start = time.time()\n",
    "    error = 0\n",
    "    error_t = 0\n",
    "    cost = 0\n",
    "    for k in range(0,n,batch_size):\n",
    "        imgs = IMGS[k:k+batch_size]\n",
    "        labels = LABS[k:k+batch_size]\n",
    "        imgs_test = IMGS_Test\n",
    "        labels_test = LABS_Test\n",
    "        w1_gd = 0\n",
    "        w2_gd = 0\n",
    "        b1_gd = 0\n",
    "        b2_gd = 0\n",
    "        t = 0\n",
    "        a2_m = 0\n",
    "        a1_m = 0\n",
    "        a2_v = 0\n",
    "        a1_v = 0\n",
    "        a2_m_b = 0\n",
    "        a1_m_b = 0\n",
    "        a2_v_b = 0\n",
    "        a1_v_b = 0\n",
    "\n",
    "        for x, y in zip(imgs, labels):\n",
    "            x = np.array(x, ndmin=2).T\n",
    "            y = np.array(y, ndmin=2).T\n",
    "            z1 = np.dot(w1, x) + b1\n",
    "            a1 = relu(z1)\n",
    "            z2 = np.dot(w2, a1) + b2\n",
    "            a2 = softmax(z2)\n",
    "\n",
    "            d2 = a2 - y\n",
    "            d1 = relu_prime(z1)*np.dot(w2.T, d2)\n",
    "\n",
    "            w1_gd += np.dot(d1, x.T)\n",
    "            w2_gd += np.dot(d2, a1.T)\n",
    "            b1_gd += d1\n",
    "            b2_gd += d2\n",
    "            error += int(np.argmax(a2) != np.argmax(y))\n",
    "            cost = np.sum(y*np.log(a2) + (1-y)*np.log(1-a2))\n",
    "            # Adam\n",
    "            t += 1\n",
    "            a2_m = a2_m * beta_1 + (1-beta_1)*w2_gd\n",
    "            a1_m = a1_m * beta_1 + (1-beta_1)*w1_gd\n",
    "            a2_m_b = a2_m_b * beta_1 + (1-beta_1)*b2_gd\n",
    "            a1_m_b = a1_m_b * beta_1 + (1-beta_1)*b1_gd\n",
    "        \n",
    "            a2_v = a2_v * beta_2 + (1-beta_2)* (w2_gd ** 2)\n",
    "            a1_v = a1_v * beta_2 + (1-beta_2)* (w1_gd ** 2)\n",
    "            a2_v_b = a2_v_b * beta_2 + (1-beta_2)* (b2_gd ** 2)\n",
    "            a1_v_b = a1_v_b * beta_2 + (1-beta_2)* (b1_gd ** 2)\n",
    "        \n",
    "            a2_m_nor = a2_m / (1 - (beta_1 ** t))\n",
    "            a1_m_nor = a1_m / (1 - (beta_1 ** t))\n",
    "            a2_v_nor = a2_v / (1 - (beta_2 ** t))\n",
    "            a1_v_nor = a1_v / (1 - (beta_2 ** t))\n",
    "            a2_m_b_nor = a2_m_b / (1 - (beta_1 ** t))\n",
    "            a1_m_b_nor = a1_m_b / (1 - (beta_1 ** t))\n",
    "            a2_v_b_nor = a2_v_b / (1 - (beta_2 ** t))\n",
    "            a1_v_b_nor = a1_v_b / (1 - (beta_2 ** t))        \n",
    "        w2 -= (a2_m_nor / (np.sqrt(a2_v_nor) + epsilon)) / batch_size\n",
    "        w1 -= (a1_m_nor / (np.sqrt(a1_v_nor) + epsilon)) / batch_size\n",
    "        b2 -= (a2_m_b_nor / (np.sqrt(a2_v_b_nor) + epsilon)) / batch_size\n",
    "        b1 -= (a1_m_b_nor / (np.sqrt(a1_v_b_nor) + epsilon)) / batch_size\n",
    "        \n",
    "#         # Gradient Descent\n",
    "#         w1 -= lr * w1_gd / batch_size\n",
    "#         w2 -= lr * w2_gd / batch_size\n",
    "#         b1 -= lr * b1_gd / batch_size\n",
    "#         b2 -= lr * b2_gd / batch_size\n",
    "    time_end = time.time()\n",
    "    print(\"Epoch: \"+str(j+1)+\"\\tCost: {:.3f}\\t\".format(-cost)+\"\\tAccuracy: {:.3f}\\t\".format(1 - error/n)+\n",
    "          \"\\tTime: {:.3f}\\t\".format(time_end-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.966\t\n"
     ]
    }
   ],
   "source": [
    "    n_test = mnist.test.num_examples\n",
    "    for x,y in zip(imgs_test, labels_test):\n",
    "        x = np.array(x, ndmin=2).T\n",
    "        y = np.array(y, ndmin=2).T\n",
    "        z1 = np.dot(w1, x) + b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(w2, a1) + b2\n",
    "        a2 = softmax(z2)\n",
    "        error_t += int(np.argmax(a2) != np.argmax(y))\n",
    "    print(\"Test Accuracy: {:.3f}\\t\".format(1 - error_t/n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show the advantage of ADAM Optimizer compare to Gradient Descent, this time I print out the result for batch size of 1024 because it performs the worst in last part. As we can see from the following table, in the case of 1024 batch size, the cost of first epoch drop to 0.589 compare to 0.943 in part e, and the test accuracy increase to 0.966. Also, we can notice that after 20 epoches of training, cost converge to 0.008 which is way better than 0.187 when we use gradient descent. Another huge change is the average training time for one epoch, it goes up to around 40 seconds in this case mainly because much more computations are brought by ADAM Optimizer. I'm still using CPU training in problem 1, so you can see although the pattern of training time is the same as last part, which goes down when increase batch size, the last one is actually bigger than size of 256. I think this might because after all these calculation, the CPU is too hot, and leads to CPU clock frequency reduction. In conclusion, ADAM improve the performance but is more complex in computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| batch size \t| cost of first epoch \t| average training time \t| test accuracy \t|\n",
    "|:----------:\t|:-------------------:\t|:---------------------:\t|:-------------:\t|\n",
    "|     16     \t|        0.000        \t|         42.389        \t|     0.977     \t|\n",
    "|     64     \t|        0.000        \t|         38.437        \t|     0.977     \t|\n",
    "|     256    \t|        0.003        \t|         35.212        \t|     0.973     \t|\n",
    "|    1024    \t|        0.589        \t|         36.308        \t|     0.966     \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part g:  \n",
    "TensorFlow implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch 1.0\tCost: 2.491\t, Accuracy= 0.297\tTime: 0.187\t\n",
      "Epoch 2.0\tCost: 0.076\t, Accuracy= 1.000\tTime: 0.845\t\n",
      "Epoch 3.0\tCost: 0.128\t, Accuracy= 1.000\tTime: 0.824\t\n",
      "Epoch 4.0\tCost: 0.158\t, Accuracy= 0.984\tTime: 0.695\t\n",
      "Epoch 5.0\tCost: 0.119\t, Accuracy= 0.984\tTime: 0.839\t\n",
      "Epoch 6.0\tCost: 0.102\t, Accuracy= 1.000\tTime: 0.851\t\n",
      "Epoch 7.0\tCost: 0.075\t, Accuracy= 1.000\tTime: 0.855\t\n",
      "Epoch 8.0\tCost: 0.094\t, Accuracy= 0.984\tTime: 0.863\t\n",
      "Epoch 9.0\tCost: 0.109\t, Accuracy= 1.000\tTime: 0.874\t\n",
      "Epoch 10.0\tCost: 0.002\t, Accuracy= 1.000\tTime: 0.890\t\n",
      "Epoch 11.0\tCost: 0.010\t, Accuracy= 1.000\tTime: 0.870\t\n",
      "Epoch 12.0\tCost: 0.030\t, Accuracy= 1.000\tTime: 0.893\t\n",
      "Epoch 13.0\tCost: 0.060\t, Accuracy= 1.000\tTime: 0.883\t\n",
      "Epoch 14.0\tCost: 0.012\t, Accuracy= 1.000\tTime: 0.890\t\n",
      "Epoch 15.0\tCost: 0.025\t, Accuracy= 1.000\tTime: 0.924\t\n",
      "Epoch 16.0\tCost: 0.019\t, Accuracy= 1.000\tTime: 0.932\t\n",
      "Epoch 17.0\tCost: 0.005\t, Accuracy= 1.000\tTime: 0.908\t\n",
      "Epoch 18.0\tCost: 0.010\t, Accuracy= 1.000\tTime: 0.910\t\n",
      "Epoch 19.0\tCost: 0.020\t, Accuracy= 1.000\tTime: 0.906\t\n",
      "Epoch 20.0\tCost: 0.063\t, Accuracy= 1.000\tTime: 0.899\t\n",
      "Training finished!\n",
      "Testing ACcuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)\n",
    "\n",
    "#learning rate\n",
    "lr = 0.2\n",
    "#number of traning epochs\n",
    "epochs = 20\n",
    "#number of batch_size\n",
    "batch_size = 64\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "num_steps = epochs * total_batch\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 100\n",
    "# n_hidden_2 = 200\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1],stddev=0.1),name='W1'),\n",
    "    # 'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.1),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_1, num_classes],stddev=0.1),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    # 'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "    # layer_2_out = tf.nn.relu(tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2']))\n",
    "    out = tf.add(tf.matmul(layer_1_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "Y_hat = tf.nn.softmax(logits)\n",
    "\n",
    "#define cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='cost')\n",
    "#define optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(Y_hat,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    time_start = time.time()\n",
    "    for i in range(num_steps):\n",
    "        # fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # run optimization\n",
    "        _, loss = sess.run([train_op, cost], feed_dict={X: batch_x, Y: batch_y})\n",
    "        if i % total_batch == 0:\n",
    "            time_end = time.time()\n",
    "            acc = sess.run(accuracy, feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Epoch \" + str(i/total_batch+1) +\"\\tCost: {:.3f}\\t\".format(loss)+\", Accuracy= {:.3f}\".format(acc)+\n",
    "                  \"\\tTime: {:.3f}\\t\".format(time_end-time_start))\n",
    "            time_start = time.time()\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    print(\"Testing ACcuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from tensorflow implementation, the training time for 1 epoch is 10 times faster than numpy implementation. Besides, it also converges much faster in tensorflow. I think this might because a lot of optimizations have been performed in tensorflow, and thus make it fast to train and converge. The test accuracy is also 0.977, which is the same as my numpy implementation. This means at least my numpy implementation works and the upper limit accuracy of this structure is around 0.977. The following table is the table for different batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| batch size \t| cost of first epoch \t| average training time \t| test accuracy \t|\n",
    "|:----------:\t|:-------------------:\t|:---------------------:\t|:-------------:\t|\n",
    "|     16     \t|        0.030        \t|         2.674         \t|     0.977     \t|\n",
    "|     64     \t|        0.057        \t|         0.867         \t|     0.977     \t|\n",
    "|     256    \t|        0.326        \t|         0.986         \t|     0.973     \t|\n",
    "|    1024    \t|        0.510        \t|         0.943         \t|     0.948     \t|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

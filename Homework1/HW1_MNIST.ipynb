{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract MNIST data</h2>\n",
    "<p style=\"font-size:20px\">You can change the option of one_hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#get mnist data, with one_hot encoding\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "#suppress warnings\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate\n",
    "lr = 0.01\n",
    "#number of traning epochs\n",
    "epochs = 20\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "num_steps = epochs * total_batch\n",
    "#network parameters\n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 200\n",
    "num_input = 784\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define placeholder and Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1], stddev=0.1),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.1),name='W2'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_2, num_classes],stddev=0.1),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define neural network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "    layer_2_out = tf.nn.relu(tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2']))\n",
    "    out = tf.add(tf.matmul(layer_2_out,weights['Wout']),biases['bout'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define cost function and accuracy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "\n",
    "#define cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='cost')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Execute training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0, Accuracy= 0.445\n",
      "Epoch 2.0, Accuracy= 0.969\n",
      "Epoch 3.0, Accuracy= 0.945\n",
      "Epoch 4.0, Accuracy= 0.992\n",
      "Epoch 5.0, Accuracy= 1.000\n",
      "Epoch 6.0, Accuracy= 1.000\n",
      "Epoch 7.0, Accuracy= 0.992\n",
      "Epoch 8.0, Accuracy= 0.984\n",
      "Epoch 9.0, Accuracy= 0.984\n",
      "Epoch 10.0, Accuracy= 0.977\n",
      "Epoch 11.0, Accuracy= 0.977\n",
      "Epoch 12.0, Accuracy= 0.992\n",
      "Epoch 13.0, Accuracy= 0.984\n",
      "Epoch 14.0, Accuracy= 0.969\n",
      "Epoch 15.0, Accuracy= 1.000\n",
      "Epoch 16.0, Accuracy= 1.000\n",
      "Epoch 17.0, Accuracy= 0.992\n",
      "Epoch 18.0, Accuracy= 0.992\n",
      "Epoch 19.0, Accuracy= 0.992\n",
      "Epoch 20.0, Accuracy= 0.984\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        # fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # run optimization\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if i % total_batch == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Epoch \" + str(i/total_batch+1) + \", Accuracy= {:.3f}\".format(acc))\n",
    "    print(\"Training finished!\")\n",
    "    result = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Your results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9722\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion  \n",
    "First, I tried to keep the structure of the NN as close as the example NN. I used a 2 hidden layer neural network, with 100 perceptrons in first hidden layer and 300 perceptron at second layre as the example. However, I add relu activate function to the first two output, and change my optimizer to ADAM. After this change, the accuracy goes up to **0.94**, but still does not meet the requirement. Therefore, I change the number of the perceptron in each layer a little bit. I keep the total number of perceptron as 400 still, but 200 for each layer. Other settings remain the same. The accuracy goes up to **0.96**. I tried to set the standard deviation to 0.1 when I initialize the weights, and the accuracy goes up again to **0.97**, as shown in the result. Then I try to add the number of the perceptron to 300 for each layer, change my activation function, change the size of batch, the number of the epoch. After several experiments, I found out no matter how I change, the accuracy stuck at 0.97. Therefore, I thought the best accuracy for a 2 layer fully connected neural network is around 0.97. Then, I started to increase the depth of the network to see how the performance will change. I build a neural network with 5 hidden layers. For that neural network, the depth of the network and the learning rate are the only things I changed, and the accuracy goes up to **0.98** as shown in the following cell. I looked up for some information and find out when implementing a real deep nerual network, let's say hundreds of hidden layers, the accuracy will go up to 0.99. However, that's unnecessary. It wastes a lot of resources to only increase 1% of the accracy. In conclusion, the active function contributes the most to the accuracy. Besides that, the number of neurals for each layer, the choice of optimizer, initialization of the weights and the depth of the network also play important roles to increase the performance of the network. The accuracy of MNIST goes form **0.84**(in example) to **0.98** after all these process, which is very impressive. In practice, we need to choose all these carefully with experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0, Accuracy= 0.242\n",
      "Epoch 2.0, Accuracy= 0.938\n",
      "Epoch 3.0, Accuracy= 0.977\n",
      "Epoch 4.0, Accuracy= 0.992\n",
      "Epoch 5.0, Accuracy= 0.984\n",
      "Epoch 6.0, Accuracy= 1.000\n",
      "Epoch 7.0, Accuracy= 1.000\n",
      "Epoch 8.0, Accuracy= 0.992\n",
      "Epoch 9.0, Accuracy= 0.992\n",
      "Epoch 10.0, Accuracy= 1.000\n",
      "Epoch 11.0, Accuracy= 1.000\n",
      "Epoch 12.0, Accuracy= 1.000\n",
      "Epoch 13.0, Accuracy= 1.000\n",
      "Epoch 14.0, Accuracy= 1.000\n",
      "Epoch 15.0, Accuracy= 1.000\n",
      "Epoch 16.0, Accuracy= 1.000\n",
      "Epoch 17.0, Accuracy= 1.000\n",
      "Epoch 18.0, Accuracy= 1.000\n",
      "Epoch 19.0, Accuracy= 1.000\n",
      "Epoch 20.0, Accuracy= 1.000\n",
      "Training finished!\n",
      "Testing ACcuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "#learning rate\n",
    "lr = 0.001\n",
    "#number of traning epochs\n",
    "epochs = 20\n",
    "#number of batch_size\n",
    "batch_size = 128\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "num_steps = epochs * total_batch\n",
    "\n",
    "#network parameters\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 200\n",
    "n_hidden_3 = 100\n",
    "n_hidden_4 = 60\n",
    "n_hidden_5 = 30\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#tf graph input\n",
    "X = tf.placeholder(tf.float32,[None,num_input],name='X')\n",
    "Y = tf.placeholder(tf.int32,[None,num_classes],name='Y')\n",
    "\n",
    "#Layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([num_input, n_hidden_1],stddev=0.1),name='W1'),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=0.1),name='W2'),\n",
    "    'W3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=0.1), name='W3'),\n",
    "    'W4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4],stddev=0.1), name='W4'),\n",
    "    'W5': tf.Variable(tf.random_normal([n_hidden_4, n_hidden_5],stddev=0.1), name='W5'),\n",
    "    'Wout': tf.Variable(tf.random_normal([n_hidden_5, num_classes],stddev=0.1),name='Wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros(shape=[n_hidden_1]),name='b1'),\n",
    "    'b2': tf.Variable(tf.zeros(shape=[n_hidden_2]),name='b2'),\n",
    "    'b3': tf.Variable(tf.zeros(shape=[n_hidden_3]),name='b3'),\n",
    "    'b4': tf.Variable(tf.zeros(shape=[n_hidden_4]),name='b4'),\n",
    "    'b5': tf.Variable(tf.zeros(shape=[n_hidden_5]), name='b5'),\n",
    "    'bout': tf.Variable(tf.zeros(shape=[num_classes]),name='bout')\n",
    "}\n",
    "\n",
    "#define a neural net model\n",
    "def neural_net(x):\n",
    "    layer_1_out = tf.nn.relu(tf.add(tf.matmul(x,weights['W1']),biases['b1']))\n",
    "    layer_2_out = tf.nn.relu(tf.add(tf.matmul(layer_1_out,weights['W2']),biases['b2']))\n",
    "    layer_3_out = tf.nn.relu(tf.add(tf.matmul(layer_2_out, weights['W3']), biases['b3']))\n",
    "    layer_4_out = tf.nn.relu(tf.add(tf.matmul(layer_3_out, weights['W4']), biases['b4']))\n",
    "    layer_5_out = tf.nn.relu(tf.add(tf.matmul(layer_4_out, weights['W5']), biases['b5']))\n",
    "    out = tf.add(tf.matmul(layer_5_out,weights['Wout']),biases['bout'])\n",
    "    return out\n",
    "\n",
    "#predicted labels\n",
    "logits = neural_net(X)\n",
    "Y_hat = tf.nn.softmax(logits)\n",
    "\n",
    "#define cost\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y),name='cost')\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "#compare the predicted labels with true labels\n",
    "correct_pred = tf.equal(tf.argmax(Y_hat,1),tf.argmax(Y,1))\n",
    "\n",
    "#compute the accuracy by taking average\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32),name='accuracy')\n",
    "\n",
    "#Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        # fetch batch\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # run optimization\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if i % total_batch == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={X: batch_x, Y: batch_y})\n",
    "            print(\"Epoch \" + str(i/total_batch+1) + \", Accuracy= {:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    print(\"Testing ACcuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
